{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphTensor(\n",
       "  context=Context(features={}, sizes=[1], shape=(), indices_dtype=tf.int32),\n",
       "  node_set_names=['user', 'dish'],\n",
       "  edge_set_names=[('user', 'interacts', 'dish')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_graph_tensor(user_features, dish_features, interaction_df):\n",
    "    \"\"\"\n",
    "    将用户-菜品数据转换为 GraphTensor\n",
    "    Args:\n",
    "        user_features: [num_users, user_feature_dim]\n",
    "        dish_features: [num_dishes, dish_feature_dim]\n",
    "        interaction_df: DataFrame with user_id, dish_id, score\n",
    "    Returns:\n",
    "        GraphTensor: TF-GNN的图数据对象\n",
    "    \"\"\"\n",
    "    num_users = user_features.shape[0]\n",
    "    num_dishes = dish_features.shape[0]\n",
    "    \n",
    "    # 创建 GraphTensor\n",
    "    graph = tfgnn.GraphTensor.from_pieces(\n",
    "        node_sets={\n",
    "            'user': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([num_users]),\n",
    "                features={'features': tf.convert_to_tensor(user_features, dtype=tf.float32)}\n",
    "            ),\n",
    "            'dish': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([num_dishes]),\n",
    "                features={'features': tf.convert_to_tensor(dish_features, dtype=tf.float32)}\n",
    "            )\n",
    "        },\n",
    "        edge_sets={\n",
    "            ('user', 'interacts', 'dish'): tfgnn.EdgeSet.from_fields(\n",
    "                sizes=tf.constant([len(interaction_df)]),\n",
    "                adjacency=tfgnn.Adjacency.from_indices(\n",
    "                    source=('user', tf.convert_to_tensor(interaction_df['user_id'].values, dtype=tf.int32)),\n",
    "                    target=('dish', tf.convert_to_tensor(interaction_df['dish_id'].values, dtype=tf.int32))\n",
    "                ),\n",
    "                features={'score': tf.convert_to_tensor(interaction_df['score'].values, dtype=tf.float32)}\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# 示例数据\n",
    "num_users, num_dishes = 100, 200\n",
    "user_feature_dim, dish_feature_dim = 16, 12\n",
    "user_features = np.random.rand(num_users, user_feature_dim)\n",
    "dish_features = np.random.rand(num_dishes, dish_feature_dim)\n",
    "interaction_data = {\n",
    "    'user_id': np.random.randint(0, num_users, 1000),\n",
    "    'dish_id': np.random.randint(0, num_dishes, 1000),\n",
    "    'score': np.random.rand(1000) * 5\n",
    "}\n",
    "interaction_df = pd.DataFrame(interaction_data)\n",
    "\n",
    "graph_tensor = prepare_graph_tensor(user_features, dish_features, interaction_df)\n",
    "\n",
    "graph_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteGNN(tf.keras.Model):\n",
    "    def __init__(self, user_dim, dish_dim, hidden_dim, output_dim):\n",
    "        super(BipartiteGNN, self).__init__()\n",
    "        \n",
    "        # 用户节点更新\n",
    "        self.user_conv1 = tfgnn.keras.layers.GraphUpdate(\n",
    "            node_sets={'user': tfgnn.keras.layers.NodeSetUpdate(\n",
    "                {'interacts': tfgnn.keras.layers.SimpleConv(\n",
    "                    sender_node_feature='features',\n",
    "                    message_fn=tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )},\n",
    "                tfgnn.keras.layers.NextStateFromConcat(\n",
    "                    tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )\n",
    "            )}\n",
    "        )\n",
    "        self.user_conv2 = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "        # 菜品节点更新\n",
    "        self.dish_conv1 = tfgnn.keras.layers.GraphUpdate(\n",
    "            node_sets={'dish': tfgnn.keras.layers.NodeSetUpdate(\n",
    "                {('user', 'interacts', 'dish'): tfgnn.keras.layers.SimpleConv(\n",
    "                    sender_node_feature='features',\n",
    "                    message_fn=tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )},\n",
    "                tfgnn.keras.layers.NextStateFromConcat(\n",
    "                    tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )\n",
    "            )}\n",
    "        )\n",
    "        self.dish_conv2 = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "    def call(self, graph):\n",
    "        # 用户更新\n",
    "        graph = self.user_conv1(graph)\n",
    "        user_emb = self.user_conv2(graph.node_sets['user']['features'])\n",
    "        \n",
    "        # 菜品更新\n",
    "        graph = self.dish_conv1(graph)\n",
    "        dish_emb = self.dish_conv2(graph.node_sets['dish']['features'])\n",
    "        \n",
    "        return user_emb, dish_emb\n",
    "\n",
    "# 初始化模型\n",
    "model = BipartiteGNN(user_feature_dim, dish_feature_dim, hidden_dim=32, output_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Exception encountered when calling layer 'node_set_update_2' (type NodeSetUpdate).\\n\\nhidden_state\\n\\nCall arguments received by layer 'node_set_update_2' (type NodeSetUpdate):\\n  • graph=GraphTensor(\\n  context=Context(features={}, sizes=[1], shape=(), indices_dtype=tf.int32),\\n  node_set_names=['user', 'dish'],\\n  edge_set_names=[('user', 'interacts', 'dish')])\\n  • node_set_name='user'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, graph, epochs, lr)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 17\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, graph)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(model, graph):\n\u001b[1;32m----> 2\u001b[0m     user_emb, dish_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     edge_indices \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39medge_sets[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteracts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdish\u001b[39m\u001b[38;5;124m'\u001b[39m)]\u001b[38;5;241m.\u001b[39madjacency\n\u001b[0;32m      4\u001b[0m     user_idx \u001b[38;5;241m=\u001b[39m edge_indices\u001b[38;5;241m.\u001b[39msource\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m, in \u001b[0;36mBipartiteGNN.call\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# 用户更新\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_conv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     user_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_conv2(graph\u001b[38;5;241m.\u001b[39mnode_sets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# 菜品更新\u001b[39;00m\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\tensorflow_gnn\\keras\\layers\\graph_update.py:252\u001b[0m, in \u001b[0;36mGraphUpdate.call\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_set_name, update_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_set_updates\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m    250\u001b[0m   features \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mnode_sets[node_set_name]\u001b[38;5;241m.\u001b[39mget_features_dict()\n\u001b[0;32m    251\u001b[0m   features\u001b[38;5;241m.\u001b[39mupdate(_ensure_dict(\n\u001b[1;32m--> 252\u001b[0m       \u001b[43mupdate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_set_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_set_name\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    253\u001b[0m   node_set_features[node_set_name] \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m    254\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mreplace_features(node_sets\u001b[38;5;241m=\u001b[39mnode_set_features)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\tensorflow_gnn\\keras\\layers\\graph_update.py:436\u001b[0m, in \u001b[0;36mNodeSetUpdate.call\u001b[1;34m(self, graph, node_set_name)\u001b[0m\n\u001b[0;32m    433\u001b[0m next_state_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Input from the nodes themselves.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m next_state_inputs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 436\u001b[0m     \u001b[43m_get_feature_or_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_set_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node_input_feature\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# Input from edge sets.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m input_from_edge_sets \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\tensorflow_gnn\\keras\\layers\\graph_update.py:575\u001b[0m, in \u001b[0;36m_get_feature_or_features\u001b[1;34m(features, names)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_feature_or_features\u001b[39m(features, names):\n\u001b[0;32m    574\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(names, const\u001b[38;5;241m.\u001b[39mFieldName):\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    576\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m names:  \u001b[38;5;66;03m# None or empty.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\tensorflow_gnn\\graph\\graph_tensor.py:55\u001b[0m, in \u001b[0;36m_GraphPieceWithFeatures.__getitem__\u001b[1;34m(self, feature_name)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_name: FieldName) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Field:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Indexing operator `[]` to access feature values by their name.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_features_ref\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Exception encountered when calling layer 'node_set_update_2' (type NodeSetUpdate).\\n\\nhidden_state\\n\\nCall arguments received by layer 'node_set_update_2' (type NodeSetUpdate):\\n  • graph=GraphTensor(\\n  context=Context(features={}, sizes=[1], shape=(), indices_dtype=tf.int32),\\n  node_set_names=['user', 'dish'],\\n  edge_set_names=[('user', 'interacts', 'dish')])\\n  • node_set_name='user'\""
     ]
    }
   ],
   "source": [
    "def compute_loss(model, graph):\n",
    "    user_emb, dish_emb = model(graph)\n",
    "    edge_indices = graph.edge_sets[('user', 'interacts', 'dish')].adjacency\n",
    "    user_idx = edge_indices.source\n",
    "    dish_idx = edge_indices.target\n",
    "    pred_scores = tf.reduce_sum(\n",
    "        tf.gather(user_emb, user_idx) * tf.gather(dish_emb, dish_idx),\n",
    "        axis=1\n",
    "    )\n",
    "    true_scores = graph.edge_sets[('user', 'interacts', 'dish')]['score']\n",
    "    return tf.keras.losses.mean_squared_error(true_scores, pred_scores)\n",
    "\n",
    "def train_model(model, graph, epochs=100, lr=0.01):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(model, graph)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.numpy():.4f}')\n",
    "    return model\n",
    "\n",
    "# 训练\n",
    "trained_model = train_model(model, graph_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Exception encountered when calling layer 'node_set_update_4' (type NodeSetUpdate).\\n\\nhidden_state\\n\\nCall arguments received by layer 'node_set_update_4' (type NodeSetUpdate):\\n  • graph=GraphTensor(\\n  context=Context(features={}, sizes=[1], shape=(), indices_dtype=tf.int32),\\n  node_set_names=['user', 'dish'],\\n  edge_set_names=[('user', 'interacts', 'dish'), ('dish', 'interacts_reverse', 'user')])\\n  • node_set_name='user'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 5 recommended dishes for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommendations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 123\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    121\u001b[0m graph_tensor \u001b[38;5;241m=\u001b[39m prepare_graph_tensor(user_features, dish_features, interaction_df)\n\u001b[0;32m    122\u001b[0m model \u001b[38;5;241m=\u001b[39m BipartiteGNN(user_feature_dim, dish_feature_dim, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    126\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_recommendations(trained_model, graph_tensor, user_id)\n",
      "Cell \u001b[1;32mIn[12], line 93\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, graph, epochs, lr)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 93\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     95\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "Cell \u001b[1;32mIn[12], line 78\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, graph)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(model, graph):\n\u001b[1;32m---> 78\u001b[0m     user_emb, dish_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     edge_indices \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39medge_sets[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteracts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdish\u001b[39m\u001b[38;5;124m'\u001b[39m)]\u001b[38;5;241m.\u001b[39madjacency\n\u001b[0;32m     80\u001b[0m     user_idx \u001b[38;5;241m=\u001b[39m edge_indices\u001b[38;5;241m.\u001b[39msource\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[12], line 70\u001b[0m, in \u001b[0;36mBipartiteGNN.call\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph):\n\u001b[1;32m---> 70\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_conv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     user_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_conv2(graph\u001b[38;5;241m.\u001b[39mnode_sets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     72\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdish_conv1(graph)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\tensorflow_gnn\\keras\\layers\\graph_update.py:252\u001b[0m, in \u001b[0;36mGraphUpdate.call\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_set_name, update_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_set_updates\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m    250\u001b[0m   features \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mnode_sets[node_set_name]\u001b[38;5;241m.\u001b[39mget_features_dict()\n\u001b[0;32m    251\u001b[0m   features\u001b[38;5;241m.\u001b[39mupdate(_ensure_dict(\n\u001b[1;32m--> 252\u001b[0m       \u001b[43mupdate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_set_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_set_name\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    253\u001b[0m   node_set_features[node_set_name] \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m    254\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mreplace_features(node_sets\u001b[38;5;241m=\u001b[39mnode_set_features)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\tensorflow_gnn\\keras\\layers\\graph_update.py:436\u001b[0m, in \u001b[0;36mNodeSetUpdate.call\u001b[1;34m(self, graph, node_set_name)\u001b[0m\n\u001b[0;32m    433\u001b[0m next_state_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Input from the nodes themselves.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m next_state_inputs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 436\u001b[0m     \u001b[43m_get_feature_or_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_set_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node_input_feature\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# Input from edge sets.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m input_from_edge_sets \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\tensorflow_gnn\\keras\\layers\\graph_update.py:575\u001b[0m, in \u001b[0;36m_get_feature_or_features\u001b[1;34m(features, names)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_feature_or_features\u001b[39m(features, names):\n\u001b[0;32m    574\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(names, const\u001b[38;5;241m.\u001b[39mFieldName):\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    576\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m names:  \u001b[38;5;66;03m# None or empty.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\tensorflow_gnn\\graph\\graph_tensor.py:55\u001b[0m, in \u001b[0;36m_GraphPieceWithFeatures.__getitem__\u001b[1;34m(self, feature_name)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_name: FieldName) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Field:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Indexing operator `[]` to access feature values by their name.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_features_ref\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Exception encountered when calling layer 'node_set_update_4' (type NodeSetUpdate).\\n\\nhidden_state\\n\\nCall arguments received by layer 'node_set_update_4' (type NodeSetUpdate):\\n  • graph=GraphTensor(\\n  context=Context(features={}, sizes=[1], shape=(), indices_dtype=tf.int32),\\n  node_set_names=['user', 'dish'],\\n  edge_set_names=[('user', 'interacts', 'dish'), ('dish', 'interacts_reverse', 'user')])\\n  • node_set_name='user'\""
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 数据准备\n",
    "def prepare_graph_tensor(user_features, dish_features, interaction_df):\n",
    "    graph = tfgnn.GraphTensor.from_pieces(\n",
    "        node_sets={\n",
    "            'user': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([user_features.shape[0]]),\n",
    "                features={'features': tf.convert_to_tensor(user_features, dtype=tf.float32)}\n",
    "            ),\n",
    "            'dish': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([dish_features.shape[0]]),\n",
    "                features={'features': tf.convert_to_tensor(dish_features, dtype=tf.float32)}\n",
    "            )\n",
    "        },\n",
    "        edge_sets={\n",
    "            ('user', 'interacts', 'dish'): tfgnn.EdgeSet.from_fields(\n",
    "                sizes=tf.constant([len(interaction_df)]),\n",
    "                adjacency=tfgnn.Adjacency.from_indices(\n",
    "                    source=('user', tf.convert_to_tensor(interaction_df['user_id'].values, dtype=tf.int32)),\n",
    "                    target=('dish', tf.convert_to_tensor(interaction_df['dish_id'].values, dtype=tf.int32))\n",
    "                ),\n",
    "                features={'score': tf.convert_to_tensor(interaction_df['score'].values, dtype=tf.float32)}\n",
    "            ),\n",
    "            ('dish', 'interacts_reverse', 'user'): tfgnn.EdgeSet.from_fields(\n",
    "                sizes=tf.constant([len(interaction_df)]),\n",
    "                adjacency=tfgnn.Adjacency.from_indices(\n",
    "                    source=('dish', tf.convert_to_tensor(interaction_df['dish_id'].values, dtype=tf.int32)),\n",
    "                    target=('user', tf.convert_to_tensor(interaction_df['user_id'].values, dtype=tf.int32))\n",
    "                ),\n",
    "                features={'score': tf.convert_to_tensor(interaction_df['score'].values, dtype=tf.float32)}\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    return graph\n",
    "\n",
    "# 模型定义\n",
    "class BipartiteGNN(tf.keras.Model):\n",
    "    def __init__(self, user_dim, dish_dim, hidden_dim, output_dim):\n",
    "        super(BipartiteGNN, self).__init__()\n",
    "        self.user_conv1 = tfgnn.keras.layers.GraphUpdate(\n",
    "            node_sets={'user': tfgnn.keras.layers.NodeSetUpdate(\n",
    "                {('dish', 'interacts_reverse', 'user'): tfgnn.keras.layers.SimpleConv(\n",
    "                    sender_node_feature='features',\n",
    "                    message_fn=tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )},\n",
    "                tfgnn.keras.layers.NextStateFromConcat(\n",
    "                    tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )\n",
    "            )}\n",
    "        )\n",
    "        self.user_conv2 = tf.keras.layers.Dense(output_dim)\n",
    "        self.dish_conv1 = tfgnn.keras.layers.GraphUpdate(\n",
    "            node_sets={'dish': tfgnn.keras.layers.NodeSetUpdate(\n",
    "                {('user', 'interacts', 'dish'): tfgnn.keras.layers.SimpleConv(\n",
    "                    sender_node_feature='features',\n",
    "                    message_fn=tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )},\n",
    "                tfgnn.keras.layers.NextStateFromConcat(\n",
    "                    tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )\n",
    "            )}\n",
    "        )\n",
    "        self.dish_conv2 = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "    def call(self, graph):\n",
    "        graph = self.user_conv1(graph)\n",
    "        user_emb = self.user_conv2(graph.node_sets['user']['features'])\n",
    "        graph = self.dish_conv1(graph)\n",
    "        dish_emb = self.dish_conv2(graph.node_sets['dish']['features'])\n",
    "        return user_emb, dish_emb\n",
    "\n",
    "# 训练和损失\n",
    "def compute_loss(model, graph):\n",
    "    user_emb, dish_emb = model(graph)\n",
    "    edge_indices = graph.edge_sets[('user', 'interacts', 'dish')].adjacency\n",
    "    user_idx = edge_indices.source\n",
    "    dish_idx = edge_indices.target\n",
    "    pred_scores = tf.reduce_sum(\n",
    "        tf.gather(user_emb, user_idx) * tf.gather(dish_emb, dish_idx),\n",
    "        axis=1\n",
    "    )\n",
    "    true_scores = graph.edge_sets[('user', 'interacts', 'dish')]['score']\n",
    "    return tf.keras.losses.mean_squared_error(true_scores, pred_scores)\n",
    "\n",
    "def train_model(model, graph, epochs=100, lr=0.01):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(model, graph)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.numpy():.4f}')\n",
    "    return model\n",
    "\n",
    "# 推荐\n",
    "def get_recommendations(model, graph, user_id, top_k=5):\n",
    "    user_emb, dish_emb = model(graph)\n",
    "    user_vec = user_emb[user_id]\n",
    "    scores = tf.matmul(dish_emb, tf.expand_dims(user_vec, 1))\n",
    "    top_indices = tf.nn.top_k(scores[:, 0], k=top_k).indices\n",
    "    return top_indices.numpy()\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    num_users, num_dishes = 100, 200\n",
    "    user_feature_dim, dish_feature_dim = 16, 12\n",
    "    user_features = np.random.rand(num_users, user_feature_dim)\n",
    "    dish_features = np.random.rand(num_dishes, dish_feature_dim)\n",
    "    interaction_data = {\n",
    "        'user_id': np.random.randint(0, num_users, 1000),\n",
    "        'dish_id': np.random.randint(0, num_dishes, 1000),\n",
    "        'score': np.random.rand(1000) * 5\n",
    "    }\n",
    "    interaction_df = pd.DataFrame(interaction_data)\n",
    "    \n",
    "    graph_tensor = prepare_graph_tensor(user_features, dish_features, interaction_df)\n",
    "    model = BipartiteGNN(user_feature_dim, dish_feature_dim, 32, 16)\n",
    "    trained_model = train_model(model, graph_tensor)\n",
    "    \n",
    "    user_id = 0\n",
    "    recommendations = get_recommendations(trained_model, graph_tensor, user_id)\n",
    "    print(f\"Top 5 recommended dishes for user {user_id}: {recommendations}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1682.5598\n",
      "Epoch 10, Loss: 36.0119\n",
      "Epoch 20, Loss: 12.9151\n",
      "Epoch 30, Loss: 5.9745\n",
      "Epoch 40, Loss: 4.2275\n",
      "Epoch 50, Loss: 3.7774\n",
      "Epoch 60, Loss: 3.5373\n",
      "Epoch 70, Loss: 3.3906\n",
      "Epoch 80, Loss: 3.2759\n",
      "Epoch 90, Loss: 3.1816\n",
      "Top 5 recommended dishes for user 0: [139 153 116 145  47]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 数据准备\n",
    "def prepare_graph_tensor(user_features, dish_features, interaction_df):\n",
    "    graph = tfgnn.GraphTensor.from_pieces(\n",
    "        node_sets={\n",
    "            'user': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([user_features.shape[0]]),\n",
    "                features={'hidden_state': tf.convert_to_tensor(user_features, dtype=tf.float32)}\n",
    "            ),\n",
    "            'dish': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([dish_features.shape[0]]),\n",
    "                features={'hidden_state': tf.convert_to_tensor(dish_features, dtype=tf.float32)}\n",
    "            )\n",
    "        },\n",
    "        edge_sets={\n",
    "            'interacts': tfgnn.EdgeSet.from_fields(\n",
    "                sizes=tf.constant([len(interaction_df)]),\n",
    "                adjacency=tfgnn.Adjacency.from_indices(\n",
    "                    source=('user', tf.convert_to_tensor(interaction_df['user_id'].values, dtype=tf.int32)),\n",
    "                    target=('dish', tf.convert_to_tensor(interaction_df['dish_id'].values, dtype=tf.int32))\n",
    "                ),\n",
    "                features={'score': tf.convert_to_tensor(interaction_df['score'].values, dtype=tf.float32)}\n",
    "            ),\n",
    "            'interacts_reverse': tfgnn.EdgeSet.from_fields(\n",
    "                sizes=tf.constant([len(interaction_df)]),\n",
    "                adjacency=tfgnn.Adjacency.from_indices(\n",
    "                    source=('dish', tf.convert_to_tensor(interaction_df['dish_id'].values, dtype=tf.int32)),\n",
    "                    target=('user', tf.convert_to_tensor(interaction_df['user_id'].values, dtype=tf.int32))\n",
    "                ),\n",
    "                features={'score': tf.convert_to_tensor(interaction_df['score'].values, dtype=tf.float32)}\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    return graph\n",
    "\n",
    "# 模型定义\n",
    "class BipartiteGNN(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(BipartiteGNN, self).__init__()\n",
    "        self.user_conv = tfgnn.keras.layers.GraphUpdate(\n",
    "            node_sets={'user': tfgnn.keras.layers.NodeSetUpdate(\n",
    "                edge_set_inputs={'interacts_reverse': tfgnn.keras.layers.SimpleConv(\n",
    "                    sender_node_feature='hidden_state',\n",
    "                    message_fn=tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )},\n",
    "                next_state=tfgnn.keras.layers.NextStateFromConcat(\n",
    "                    tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )\n",
    "            )}\n",
    "        )\n",
    "        self.dish_conv = tfgnn.keras.layers.GraphUpdate(\n",
    "            node_sets={'dish': tfgnn.keras.layers.NodeSetUpdate(\n",
    "                edge_set_inputs={'interacts': tfgnn.keras.layers.SimpleConv(\n",
    "                    sender_node_feature='hidden_state',\n",
    "                    message_fn=tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )},\n",
    "                next_state=tfgnn.keras.layers.NextStateFromConcat(\n",
    "                    tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )\n",
    "            )}\n",
    "        )\n",
    "        self.user_proj = tf.keras.layers.Dense(output_dim)\n",
    "        self.dish_proj = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "    def call(self, graph):\n",
    "        graph = self.user_conv(graph)\n",
    "        user_emb = self.user_proj(graph.node_sets['user']['hidden_state'])\n",
    "        graph = self.dish_conv(graph)\n",
    "        dish_emb = self.dish_proj(graph.node_sets['dish']['hidden_state'])\n",
    "        return user_emb, dish_emb\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(model, graph):\n",
    "    user_emb, dish_emb = model(graph)\n",
    "    edge_indices = graph.edge_sets['interacts'].adjacency\n",
    "    user_idx = edge_indices.source\n",
    "    dish_idx = edge_indices.target\n",
    "    pred_scores = tf.reduce_sum(\n",
    "        tf.gather(user_emb, user_idx) * tf.gather(dish_emb, dish_idx),\n",
    "        axis=1\n",
    "    )\n",
    "    true_scores = graph.edge_sets['interacts']['score']\n",
    "    return tf.reduce_mean(tf.keras.losses.mean_squared_error(true_scores, pred_scores))\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, graph, epochs=100, lr=0.01):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(model, graph)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.numpy():.4f}')\n",
    "    return model\n",
    "\n",
    "# 推荐系统\n",
    "def get_recommendations(model, graph, user_id, top_k=5):\n",
    "    user_emb, dish_emb = model(graph)\n",
    "    user_vec = tf.nn.l2_normalize(user_emb[user_id], axis=0)\n",
    "    dish_emb = tf.nn.l2_normalize(dish_emb, axis=1)\n",
    "    scores = tf.matmul(dish_emb, tf.expand_dims(user_vec, 1))[:, 0]\n",
    "    top_indices = tf.argsort(scores, direction='DESCENDING')[:top_k]\n",
    "    return top_indices.numpy()\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    num_users, num_dishes = 100, 200\n",
    "    user_feature_dim, dish_feature_dim = 16, 12\n",
    "    user_features = np.random.rand(num_users, user_feature_dim)\n",
    "    dish_features = np.random.rand(num_dishes, dish_feature_dim)\n",
    "    interaction_data = {\n",
    "        'user_id': np.random.randint(0, num_users, 1000),\n",
    "        'dish_id': np.random.randint(0, num_dishes, 1000),\n",
    "        'score': np.random.rand(1000) * 5\n",
    "    }\n",
    "    interaction_df = pd.DataFrame(interaction_data)\n",
    "    \n",
    "    graph_tensor = prepare_graph_tensor(user_features, dish_features, interaction_df)\n",
    "    model = BipartiteGNN(32, 16)\n",
    "    trained_model = train_model(model, graph_tensor)\n",
    "    \n",
    "    user_id = 0\n",
    "    recommendations = get_recommendations(trained_model, graph_tensor, user_id)\n",
    "    print(f\"Top 5 recommended dishes for user {user_id}: {recommendations}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爱吃辣，爱吃咸\n",
      "辣，甜\n",
      "咸\n",
      "辣\n",
      "甜\n",
      "不辣\n",
      "苦\n",
      "甜\n",
      "咸\n",
      "辣\n",
      "['辣', '酸,辣', '麻辣', '甜', '咸', '苦', '辣', '甜', '咸', '辣']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# 输入数据\n",
    "user_data = [\n",
    "    {'user_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'user_feature': ['爱吃辣，爱吃咸', '辣，甜', '咸', '辣', '甜', '不辣', '苦', '甜', '咸', '辣']}\n",
    "]\n",
    "\n",
    "dish_data = [\n",
    "    {'dish_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'dish_feature': ['辣', '酸,辣', '麻辣', '甜', '咸', '苦', '辣', '甜', '咸', '辣']}\n",
    "]\n",
    "\n",
    "interaction_data = [\n",
    "    {'user_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'dish_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'score': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1]}\n",
    "]\n",
    "\n",
    "def get_data(user_data, dish_data, interaction_data):\n",
    "    \"\"\"\n",
    "    处理用户、菜品和交互数据，生成适合 GNN 的输入格式\n",
    "    Args:\n",
    "        user_data: 用户数据列表\n",
    "        dish_data: 菜品数据列表\n",
    "        interaction_data: 交互数据列表\n",
    "    Returns:\n",
    "        user_features: 用户特征矩阵 [num_users, feature_dim]\n",
    "        dish_features: 菜品特征矩阵 [num_dishes, feature_dim]\n",
    "        interaction_df: 交互 DataFrame\n",
    "    \"\"\"\n",
    "    # 提取用户特征\n",
    "    user_features = user_data[1]['user_feature']\n",
    "    for j in user_features:\n",
    "        print(j)\n",
    "\n",
    "    \n",
    "    # 提取菜品特征\n",
    "    dish_features = dish_data[1]['dish_feature']\n",
    "    print(dish_features)\n",
    "    embeddings = OllamaEmbeddings(model=\"smartcreation/bge-large-zh-v1.5:latest\")\n",
    "\n",
    "    user_feature = [].append(embeddings.embed_query(j) for j in user_features)\n",
    "    dish_feature = [].append(embeddings.embed_query(j) for j in dish_features)\n",
    "\n",
    "    \n",
    "    # 处理交互数据\n",
    "    interaction_df = pd.DataFrame({\n",
    "        'user_id': interaction_data[0]['user_id'],\n",
    "        'dish_id': interaction_data[1]['dish_id'],\n",
    "        'score': interaction_data[2]['score']\n",
    "    })\n",
    "    \n",
    "    return user_feature, dish_feature, interaction_df\n",
    "\n",
    "# 调用函数\n",
    "user_feature, dish_feature, interaction_df = get_data(user_data, dish_data, interaction_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphTensor(\n",
       "  context=Context(features={}, sizes=[1], shape=(), indices_dtype=tf.int32),\n",
       "  node_set_names=['user', 'dish'],\n",
       "  edge_set_names=[('user', 'interacts', 'dish')])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_graph_tensor(user_features, dish_features, interaction_df):\n",
    "    \"\"\"\n",
    "    将用户-菜品数据转换为 GraphTensor\n",
    "    Args:\n",
    "        user_features: [num_users, user_feature_dim]\n",
    "        dish_features: [num_dishes, dish_feature_dim]\n",
    "        interaction_df: DataFrame with user_id, dish_id, score\n",
    "    Returns:\n",
    "        GraphTensor: TF-GNN的图数据对象\n",
    "    \"\"\"\n",
    "    num_users = user_features.shape[0]\n",
    "    num_dishes = dish_features.shape[0]\n",
    "    \n",
    "    # 创建 GraphTensor\n",
    "    graph = tfgnn.GraphTensor.from_pieces(\n",
    "        node_sets={\n",
    "            'user': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([num_users]),\n",
    "                features={'features': tf.convert_to_tensor(user_features, dtype=tf.float32)}\n",
    "            ),\n",
    "            'dish': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([num_dishes]),\n",
    "                features={'features': tf.convert_to_tensor(dish_features, dtype=tf.float32)}\n",
    "            )\n",
    "        },\n",
    "        edge_sets={\n",
    "            ('user', 'interacts', 'dish'): tfgnn.EdgeSet.from_fields(\n",
    "                sizes=tf.constant([len(interaction_df)]),\n",
    "                adjacency=tfgnn.Adjacency.from_indices(\n",
    "                    source=('user', tf.convert_to_tensor(interaction_df['user_id'].values, dtype=tf.int32)),\n",
    "                    target=('dish', tf.convert_to_tensor(interaction_df['dish_id'].values, dtype=tf.int32))\n",
    "                ),\n",
    "                features={'score': tf.convert_to_tensor(interaction_df['score'].values, dtype=tf.float32)}\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# 示例数据\n",
    "num_users, num_dishes = 100, 200\n",
    "user_feature_dim, dish_feature_dim = 16, 12\n",
    "user_features = np.random.rand(num_users, user_feature_dim)\n",
    "dish_features = np.random.rand(num_dishes, dish_feature_dim)\n",
    "interaction_data = {\n",
    "    'user_id': np.random.randint(0, num_users, 1000),\n",
    "    'dish_id': np.random.randint(0, num_dishes, 1000),\n",
    "    'score': np.random.rand(1000) * 5\n",
    "}\n",
    "interaction_df = pd.DataFrame(interaction_data)\n",
    "\n",
    "graph_tensor = prepare_graph_tensor(user_features, dish_features, interaction_df)\n",
    "\n",
    "graph_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m user_feature, dish_feature, interaction_df\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 调用函数\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m user_feature, dish_feature, interaction_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdish_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteraction_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 29\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(user_data, dish_data, interaction_data)\u001b[0m\n\u001b[0;32m     27\u001b[0m dish_features \u001b[38;5;241m=\u001b[39m dish_data[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdish_feature\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmartcreation/bge-large-zh-v1.5:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m user_feature\u001b[38;5;241m=\u001b[39m[], dish_feature\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m user_features:\n\u001b[0;32m     31\u001b[0m     user_feature \u001b[38;5;241m=\u001b[39m user_features\u001b[38;5;241m.\u001b[39mappend(embeddings\u001b[38;5;241m.\u001b[39membed_query(j))\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# 输入数据\n",
    "user_data = [\n",
    "    {'user_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'user_feature': ['爱吃辣，爱吃咸', '辣，甜', '咸', '辣', '甜', '不辣', '苦', '甜', '咸', '辣']}\n",
    "]\n",
    "\n",
    "dish_data = [\n",
    "    {'dish_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'dish_feature': ['辣', '酸,辣', '麻辣', '甜', '咸', '苦', '辣', '甜', '咸', '辣']}\n",
    "]\n",
    "\n",
    "interaction_data = [\n",
    "    {'user_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'dish_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'score': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1]}\n",
    "]\n",
    "\n",
    "def get_data(user_data, dish_data, interaction_data):\n",
    "\n",
    "    # 提取用户特征\n",
    "    user_features = user_data[1]['user_feature']  \n",
    "    # 提取菜品特征\n",
    "    dish_features = dish_data[1]['dish_feature']\n",
    "    embeddings = OllamaEmbeddings(model=\"smartcreation/bge-large-zh-v1.5:latest\")\n",
    "    user_feature=[], dish_feature=[]\n",
    "    for j in user_features:\n",
    "        user_feature = user_features.append(embeddings.embed_query(j))\n",
    "    for j in dish_features:\n",
    "        dish_feature = dish_feature.append(embeddings.embed_query(j))\n",
    "\n",
    "    \n",
    "    # 处理交互数据\n",
    "    interaction_df = pd.DataFrame({\n",
    "        'user_id': interaction_data[0]['user_id'],\n",
    "        'dish_id': interaction_data[1]['dish_id'],\n",
    "        'score': interaction_data[2]['score']\n",
    "    })\n",
    "    \n",
    "    return user_feature, dish_feature, interaction_df\n",
    "\n",
    "# 调用函数\n",
    "user_feature, dish_feature, interaction_df = get_data(user_data, dish_data, interaction_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# 输入数据\n",
    "user_features = ['爱吃辣，爱吃咸', '辣，甜', '咸', '辣', '甜', '不辣', '苦', '甜', '咸', '辣']\n",
    "\n",
    "\n",
    "dish_features = ['辣', '酸,辣', '麻辣', '甜', '咸', '苦', '辣', '甜', '咸', '辣']\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"smartcreation/bge-large-zh-v1.5:latest\")\n",
    "user_feature, dish_feature = [],[]\n",
    "for j in user_features:\n",
    "        user_feature.append(embeddings.embed_query(j))\n",
    "for j in dish_features:\n",
    "        dish_feature.append(embeddings.embed_query(j))\n",
    "\n",
    "\n",
    "interaction_data = [\n",
    "    {'user_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'dish_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {'score': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1]}\n",
    "]\n",
    "\n",
    "# 处理交互数据\n",
    "interaction_df = pd.DataFrame({\n",
    "        'user_id': interaction_data[0]['user_id'],\n",
    "        'dish_id': interaction_data[1]['dish_id'],\n",
    "        'score': interaction_data[2]['score']\n",
    "})\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "余弦相似度: 0.9678800388809308\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"smartcreation/bge-large-zh-v1.5:latest\")\n",
    "\n",
    "a=embeddings.embed_query('爱吃川菜')\n",
    "b=embeddings.embed_query('喜欢吃川菜')\n",
    "\n",
    "# 计算余弦相似度\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(a, b)\n",
    "print(f\"余弦相似度: {similarity}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7843, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shapexx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in d:\\nlp\\dpr1\\.venv\\lib\\site-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in d:\\nlp\\dpr1\\.venv\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "! pip install python-docx\n",
    "from docx import Document\n",
    "\n",
    "# 创建 Word 文档\n",
    "doc = Document()\n",
    "\n",
    "# 添加标题\n",
    "doc.add_heading('JSON 数据转换结果', level=1)\n",
    "\n",
    "# 逐行写入数据\n",
    "for index, row in df.iterrows():\n",
    "    doc.add_paragraph(f\"记录 {index + 1}:\")\n",
    "    for col_name, value in row.items():\n",
    "        doc.add_paragraph(f\"{col_name}: {value}\")\n",
    "    doc.add_paragraph(\"---\")  # 分隔线\n",
    "\n",
    "# 保存文档\n",
    "doc.save('output.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈飞扬\\AppData\\Local\\Temp\\ipykernel_15520\\822522991.py:43: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"smartcreation/bge-large-zh-v1.5:latest\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m dish_feature \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m dish_features:\n\u001b[1;32m---> 46\u001b[0m     dish_feature\u001b[38;5;241m.\u001b[39mappend(\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m dish_feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(dish_feature)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(dish_feature\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\ollama.py:227\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed a query using a Ollama deployed embedding model.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    Embeddings for the text.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m instruction_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 227\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstruction_pair\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_emb_response(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\ollama.py:167\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    164\u001b[0m }\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32md:\\NLP\\DPR1\\.venv\\lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "def extract_features(json_file_path):\n",
    "    \"\"\"\n",
    "    从 JSON 文件中读取数据，提取 taste 和 categories 作为特征\n",
    "    \n",
    "    参数:\n",
    "    json_file_path (str): JSON 文件路径\n",
    "    \n",
    "    返回:\n",
    "    dict: 包含 taste 和 categories 的特征字典\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 打开并读取 JSON 文件\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        # 提取 taste 和 categories\n",
    "        features=[]\n",
    "        names=[]\n",
    "        for i in data:\n",
    "            features.append(i['taste'])\n",
    "            features.append(i['categories'])\n",
    "            names.append(i['title'])\n",
    "        return features,names\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：文件 {json_file_path} 未找到\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"错误：文件 {json_file_path} 不是有效的 JSON 格式\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"发生未知错误：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设 JSON 数据已保存为 'recipe.json'\n",
    "    json_file = \"all_recipe.json\"\n",
    "    # 调用函数\n",
    "    dish_features,names= extract_features(json_file)\n",
    "    embeddings = OllamaEmbeddings(model=\"smartcreation/bge-large-zh-v1.5:latest\")\n",
    "    dish_feature = []\n",
    "    for j in dish_features:\n",
    "        dish_feature.append(embeddings.embed_query(j))\n",
    "    dish_feature = np.array(dish_feature)\n",
    "    print(dish_feature.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7843\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "def extract_features(json_file_path):\n",
    "    \"\"\"\n",
    "    从 JSON 文件中读取数据，提取 taste 和 categories 作为特征\n",
    "    \n",
    "    参数:\n",
    "    json_file_path (str): JSON 文件路径\n",
    "    \n",
    "    返回:\n",
    "    dict: 包含 taste 和 categories 的特征字典\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 打开并读取 JSON 文件\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        # 提取 taste 和 categories\n",
    "        print(len(data))\n",
    "        features=[]\n",
    "        names=[]\n",
    "        for i in data:\n",
    "            features.append(i['taste'])\n",
    "            features.append(i['categories'])\n",
    "            names.append(i['title'])\n",
    "        return features,names\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：文件 {json_file_path} 未找到\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"错误：文件 {json_file_path} 不是有效的 JSON 格式\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"发生未知错误：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "a,b=extract_features('all_recipe.json')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15686"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dish_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All available features: ['川菜', '湘菜', '粤菜', '东北菜', '沪菜', '鲁菜', '甜', '咸', '酸', '辣', '麻辣', '苦', '清淡']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['辣+东北菜',\n",
       " '清淡+湘菜',\n",
       " '麻辣+川菜',\n",
       " '酸+湘菜',\n",
       " '酸+东北菜',\n",
       " '辣+粤菜',\n",
       " '清淡+鲁菜',\n",
       " '苦+鲁菜',\n",
       " '苦+粤菜',\n",
       " '咸+鲁菜',\n",
       " '咸+东北菜',\n",
       " '咸+东北菜',\n",
       " '咸+川菜',\n",
       " '甜+鲁菜',\n",
       " '清淡+东北菜',\n",
       " '甜+鲁菜',\n",
       " '咸+东北菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+湘菜',\n",
       " '苦+粤菜',\n",
       " '清淡+鲁菜',\n",
       " '清淡+沪菜',\n",
       " '咸+湘菜',\n",
       " '咸+湘菜',\n",
       " '酸+川菜',\n",
       " '清淡+川菜',\n",
       " '清淡+湘菜',\n",
       " '咸+东北菜',\n",
       " '酸+湘菜',\n",
       " '咸+湘菜',\n",
       " '清淡+东北菜',\n",
       " '酸+粤菜',\n",
       " '甜+东北菜',\n",
       " '麻辣+鲁菜',\n",
       " '咸+川菜',\n",
       " '苦+湘菜',\n",
       " '辣+湘菜',\n",
       " '酸+东北菜',\n",
       " '辣+川菜',\n",
       " '清淡+川菜',\n",
       " '麻辣+沪菜',\n",
       " '辣+鲁菜',\n",
       " '辣+沪菜',\n",
       " '甜+沪菜',\n",
       " '苦+粤菜',\n",
       " '麻辣+东北菜',\n",
       " '辣+鲁菜',\n",
       " '酸+粤菜',\n",
       " '酸+鲁菜',\n",
       " '清淡+粤菜',\n",
       " '辣+川菜',\n",
       " '苦+粤菜',\n",
       " '清淡+东北菜',\n",
       " '辣+川菜',\n",
       " '酸+川菜',\n",
       " '咸+沪菜',\n",
       " '咸+鲁菜',\n",
       " '清淡+东北菜',\n",
       " '苦+湘菜',\n",
       " '甜+川菜',\n",
       " '辣+沪菜',\n",
       " '麻辣+粤菜',\n",
       " '酸+湘菜',\n",
       " '咸+东北菜',\n",
       " '苦+鲁菜',\n",
       " '苦+湘菜',\n",
       " '甜+粤菜',\n",
       " '辣+沪菜',\n",
       " '酸+沪菜',\n",
       " '咸+沪菜',\n",
       " '咸+湘菜',\n",
       " '甜+鲁菜',\n",
       " '咸+粤菜',\n",
       " '麻辣+湘菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+川菜',\n",
       " '苦+东北菜',\n",
       " '麻辣+沪菜',\n",
       " '苦+湘菜',\n",
       " '苦+东北菜',\n",
       " '清淡+川菜',\n",
       " '清淡+粤菜',\n",
       " '酸+东北菜',\n",
       " '甜+东北菜',\n",
       " '麻辣+东北菜',\n",
       " '辣+湘菜',\n",
       " '辣+沪菜',\n",
       " '清淡+川菜',\n",
       " '咸+东北菜',\n",
       " '辣+沪菜',\n",
       " '清淡+东北菜',\n",
       " '清淡+粤菜',\n",
       " '麻辣+鲁菜',\n",
       " '麻辣+沪菜',\n",
       " '甜+东北菜',\n",
       " '咸+鲁菜',\n",
       " '甜+东北菜',\n",
       " '酸+粤菜',\n",
       " '苦+鲁菜',\n",
       " '苦+东北菜',\n",
       " '甜+川菜',\n",
       " '苦+粤菜',\n",
       " '清淡+川菜',\n",
       " '苦+川菜',\n",
       " '咸+东北菜',\n",
       " '清淡+湘菜',\n",
       " '咸+粤菜',\n",
       " '清淡+湘菜',\n",
       " '辣+沪菜',\n",
       " '麻辣+沪菜',\n",
       " '咸+粤菜',\n",
       " '清淡+鲁菜',\n",
       " '辣+沪菜',\n",
       " '咸+湘菜',\n",
       " '苦+川菜',\n",
       " '清淡+川菜',\n",
       " '清淡+沪菜',\n",
       " '甜+沪菜',\n",
       " '酸+沪菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+鲁菜',\n",
       " '清淡+东北菜',\n",
       " '酸+川菜',\n",
       " '苦+东北菜',\n",
       " '清淡+粤菜',\n",
       " '清淡+东北菜',\n",
       " '清淡+川菜',\n",
       " '辣+沪菜',\n",
       " '酸+川菜',\n",
       " '甜+鲁菜',\n",
       " '甜+鲁菜',\n",
       " '酸+东北菜',\n",
       " '咸+沪菜',\n",
       " '甜+粤菜',\n",
       " '甜+湘菜',\n",
       " '甜+东北菜',\n",
       " '清淡+粤菜',\n",
       " '苦+湘菜',\n",
       " '清淡+川菜',\n",
       " '清淡+东北菜',\n",
       " '清淡+川菜',\n",
       " '咸+川菜',\n",
       " '辣+东北菜',\n",
       " '甜+川菜',\n",
       " '咸+鲁菜',\n",
       " '苦+粤菜',\n",
       " '酸+粤菜',\n",
       " '辣+川菜',\n",
       " '咸+粤菜',\n",
       " '苦+东北菜',\n",
       " '酸+沪菜',\n",
       " '甜+川菜',\n",
       " '咸+东北菜',\n",
       " '甜+东北菜',\n",
       " '辣+鲁菜',\n",
       " '甜+湘菜',\n",
       " '酸+川菜',\n",
       " '咸+鲁菜',\n",
       " '酸+粤菜',\n",
       " '辣+沪菜',\n",
       " '辣+川菜',\n",
       " '苦+粤菜',\n",
       " '苦+东北菜',\n",
       " '咸+东北菜',\n",
       " '清淡+湘菜',\n",
       " '酸+东北菜',\n",
       " '甜+粤菜',\n",
       " '咸+鲁菜',\n",
       " '甜+东北菜',\n",
       " '麻辣+湘菜',\n",
       " '麻辣+川菜',\n",
       " '麻辣+鲁菜',\n",
       " '酸+粤菜',\n",
       " '清淡+湘菜',\n",
       " '清淡+粤菜',\n",
       " '甜+湘菜',\n",
       " '麻辣+鲁菜',\n",
       " '清淡+鲁菜',\n",
       " '咸+粤菜',\n",
       " '苦+粤菜',\n",
       " '甜+鲁菜',\n",
       " '苦+东北菜',\n",
       " '甜+川菜',\n",
       " '酸+东北菜',\n",
       " '辣+川菜',\n",
       " '麻辣+东北菜',\n",
       " '辣+粤菜',\n",
       " '清淡+东北菜',\n",
       " '清淡+东北菜',\n",
       " '甜+湘菜',\n",
       " '辣+沪菜',\n",
       " '甜+沪菜',\n",
       " '清淡+粤菜',\n",
       " '甜+湘菜',\n",
       " '苦+湘菜',\n",
       " '甜+东北菜',\n",
       " '甜+湘菜',\n",
       " '辣+东北菜',\n",
       " '苦+湘菜',\n",
       " '麻辣+湘菜',\n",
       " '苦+川菜',\n",
       " '咸+川菜',\n",
       " '清淡+东北菜',\n",
       " '苦+粤菜',\n",
       " '咸+湘菜',\n",
       " '苦+川菜',\n",
       " '辣+川菜',\n",
       " '麻辣+粤菜',\n",
       " '酸+川菜',\n",
       " '苦+川菜',\n",
       " '苦+鲁菜',\n",
       " '辣+湘菜',\n",
       " '咸+湘菜',\n",
       " '麻辣+湘菜',\n",
       " '辣+鲁菜',\n",
       " '苦+川菜',\n",
       " '辣+川菜',\n",
       " '咸+沪菜',\n",
       " '咸+湘菜',\n",
       " '麻辣+东北菜',\n",
       " '苦+鲁菜',\n",
       " '苦+沪菜',\n",
       " '咸+川菜',\n",
       " '苦+湘菜',\n",
       " '酸+川菜',\n",
       " '麻辣+鲁菜',\n",
       " '苦+东北菜',\n",
       " '麻辣+沪菜',\n",
       " '咸+川菜',\n",
       " '咸+鲁菜',\n",
       " '清淡+沪菜',\n",
       " '麻辣+鲁菜',\n",
       " '辣+鲁菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+川菜',\n",
       " '麻辣+沪菜',\n",
       " '酸+粤菜',\n",
       " '酸+湘菜',\n",
       " '清淡+沪菜',\n",
       " '咸+鲁菜',\n",
       " '咸+湘菜',\n",
       " '苦+东北菜',\n",
       " '辣+湘菜',\n",
       " '辣+鲁菜',\n",
       " '甜+沪菜',\n",
       " '甜+川菜',\n",
       " '麻辣+鲁菜',\n",
       " '苦+东北菜',\n",
       " '苦+沪菜',\n",
       " '清淡+东北菜',\n",
       " '辣+粤菜',\n",
       " '酸+粤菜',\n",
       " '麻辣+湘菜',\n",
       " '辣+湘菜',\n",
       " '咸+东北菜',\n",
       " '酸+粤菜',\n",
       " '甜+川菜',\n",
       " '甜+粤菜',\n",
       " '咸+鲁菜',\n",
       " '辣+川菜',\n",
       " '咸+沪菜',\n",
       " '麻辣+粤菜',\n",
       " '辣+鲁菜',\n",
       " '酸+湘菜',\n",
       " '苦+粤菜',\n",
       " '辣+鲁菜',\n",
       " '麻辣+湘菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+粤菜',\n",
       " '清淡+鲁菜',\n",
       " '甜+湘菜',\n",
       " '清淡+湘菜',\n",
       " '苦+湘菜',\n",
       " '麻辣+粤菜',\n",
       " '苦+鲁菜',\n",
       " '辣+东北菜',\n",
       " '甜+湘菜',\n",
       " '麻辣+沪菜',\n",
       " '清淡+湘菜',\n",
       " '辣+粤菜',\n",
       " '甜+东北菜',\n",
       " '麻辣+东北菜',\n",
       " '苦+粤菜',\n",
       " '清淡+川菜',\n",
       " '甜+沪菜',\n",
       " '甜+川菜',\n",
       " '清淡+鲁菜',\n",
       " '苦+东北菜',\n",
       " '清淡+东北菜',\n",
       " '咸+东北菜',\n",
       " '辣+沪菜',\n",
       " '苦+沪菜',\n",
       " '清淡+东北菜',\n",
       " '辣+鲁菜',\n",
       " '甜+沪菜',\n",
       " '甜+东北菜',\n",
       " '甜+东北菜',\n",
       " '清淡+粤菜',\n",
       " '苦+东北菜',\n",
       " '辣+鲁菜',\n",
       " '咸+川菜',\n",
       " '咸+川菜',\n",
       " '甜+沪菜',\n",
       " '辣+鲁菜',\n",
       " '苦+粤菜',\n",
       " '酸+湘菜',\n",
       " '苦+东北菜',\n",
       " '苦+鲁菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+川菜',\n",
       " '苦+东北菜',\n",
       " '酸+湘菜',\n",
       " '咸+川菜',\n",
       " '咸+沪菜',\n",
       " '苦+粤菜',\n",
       " '咸+川菜',\n",
       " '清淡+粤菜',\n",
       " '咸+湘菜',\n",
       " '清淡+东北菜',\n",
       " '清淡+粤菜',\n",
       " '清淡+川菜',\n",
       " '甜+川菜',\n",
       " '辣+川菜',\n",
       " '清淡+东北菜',\n",
       " '辣+湘菜',\n",
       " '苦+鲁菜',\n",
       " '清淡+粤菜',\n",
       " '清淡+东北菜',\n",
       " '辣+鲁菜',\n",
       " '苦+湘菜',\n",
       " '甜+鲁菜',\n",
       " '麻辣+川菜',\n",
       " '苦+粤菜',\n",
       " '甜+川菜',\n",
       " '辣+川菜',\n",
       " '咸+沪菜',\n",
       " '麻辣+湘菜',\n",
       " '酸+湘菜',\n",
       " '苦+粤菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+川菜',\n",
       " '酸+沪菜',\n",
       " '苦+东北菜',\n",
       " '甜+沪菜',\n",
       " '辣+鲁菜',\n",
       " '甜+粤菜',\n",
       " '咸+川菜',\n",
       " '清淡+川菜',\n",
       " '咸+川菜',\n",
       " '苦+川菜',\n",
       " '甜+沪菜',\n",
       " '苦+鲁菜',\n",
       " '麻辣+鲁菜',\n",
       " '苦+湘菜',\n",
       " '清淡+川菜',\n",
       " '清淡+东北菜',\n",
       " '辣+川菜',\n",
       " '苦+鲁菜',\n",
       " '麻辣+东北菜',\n",
       " '咸+沪菜',\n",
       " '甜+东北菜',\n",
       " '辣+鲁菜',\n",
       " '甜+粤菜',\n",
       " '咸+鲁菜',\n",
       " '咸+沪菜',\n",
       " '甜+东北菜',\n",
       " '咸+粤菜',\n",
       " '辣+沪菜',\n",
       " '清淡+川菜',\n",
       " '咸+沪菜',\n",
       " '辣+川菜',\n",
       " '麻辣+川菜',\n",
       " '咸+湘菜',\n",
       " '苦+川菜',\n",
       " '酸+沪菜',\n",
       " '咸+川菜',\n",
       " '甜+鲁菜',\n",
       " '麻辣+鲁菜',\n",
       " '甜+粤菜',\n",
       " '酸+粤菜',\n",
       " '辣+鲁菜',\n",
       " '麻辣+川菜',\n",
       " '苦+湘菜',\n",
       " '咸+沪菜',\n",
       " '甜+湘菜',\n",
       " '苦+东北菜',\n",
       " '咸+鲁菜',\n",
       " '咸+鲁菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+粤菜',\n",
       " '苦+鲁菜',\n",
       " '辣+沪菜',\n",
       " '咸+湘菜',\n",
       " '麻辣+沪菜',\n",
       " '苦+川菜',\n",
       " '咸+湘菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+川菜',\n",
       " '咸+鲁菜',\n",
       " '酸+川菜',\n",
       " '咸+鲁菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+川菜',\n",
       " '清淡+沪菜',\n",
       " '甜+沪菜',\n",
       " '清淡+川菜',\n",
       " '酸+东北菜',\n",
       " '麻辣+鲁菜',\n",
       " '麻辣+鲁菜',\n",
       " '辣+鲁菜',\n",
       " '咸+东北菜',\n",
       " '甜+鲁菜',\n",
       " '酸+鲁菜',\n",
       " '麻辣+鲁菜',\n",
       " '清淡+东北菜',\n",
       " '麻辣+粤菜',\n",
       " '麻辣+沪菜',\n",
       " '清淡+沪菜',\n",
       " '清淡+鲁菜',\n",
       " '酸+鲁菜',\n",
       " '甜+湘菜',\n",
       " '麻辣+东北菜',\n",
       " '苦+粤菜',\n",
       " '甜+粤菜',\n",
       " '辣+沪菜',\n",
       " '清淡+川菜',\n",
       " '酸+川菜',\n",
       " '辣+粤菜',\n",
       " '清淡+川菜',\n",
       " '辣+粤菜',\n",
       " '麻辣+湘菜',\n",
       " '甜+鲁菜',\n",
       " '清淡+鲁菜',\n",
       " '清淡+东北菜',\n",
       " '麻辣+粤菜',\n",
       " '清淡+沪菜',\n",
       " '甜+沪菜',\n",
       " '辣+川菜',\n",
       " '甜+川菜',\n",
       " '苦+湘菜',\n",
       " '麻辣+湘菜',\n",
       " '清淡+沪菜',\n",
       " '甜+鲁菜',\n",
       " '咸+川菜',\n",
       " '咸+东北菜',\n",
       " '清淡+沪菜',\n",
       " '清淡+湘菜',\n",
       " '甜+东北菜',\n",
       " '清淡+粤菜',\n",
       " '咸+川菜',\n",
       " '甜+鲁菜',\n",
       " '苦+鲁菜',\n",
       " '清淡+鲁菜',\n",
       " '苦+湘菜',\n",
       " '麻辣+湘菜',\n",
       " '咸+鲁菜',\n",
       " '苦+川菜',\n",
       " '咸+粤菜',\n",
       " '辣+鲁菜',\n",
       " '酸+鲁菜',\n",
       " '酸+粤菜',\n",
       " '麻辣+东北菜',\n",
       " '苦+川菜',\n",
       " '麻辣+粤菜',\n",
       " '咸+沪菜',\n",
       " '甜+鲁菜',\n",
       " '酸+鲁菜',\n",
       " '辣+沪菜',\n",
       " '酸+鲁菜',\n",
       " '辣+川菜',\n",
       " '辣+东北菜',\n",
       " '酸+川菜',\n",
       " '清淡+东北菜',\n",
       " '酸+湘菜',\n",
       " '辣+川菜',\n",
       " '咸+川菜',\n",
       " '甜+东北菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+粤菜',\n",
       " '咸+沪菜',\n",
       " '苦+鲁菜',\n",
       " '辣+鲁菜',\n",
       " '苦+川菜',\n",
       " '咸+沪菜',\n",
       " '苦+湘菜',\n",
       " '清淡+沪菜',\n",
       " '甜+粤菜',\n",
       " '麻辣+粤菜',\n",
       " '甜+沪菜',\n",
       " '麻辣+沪菜',\n",
       " '辣+川菜',\n",
       " '咸+湘菜',\n",
       " '甜+川菜',\n",
       " '甜+鲁菜',\n",
       " '辣+沪菜',\n",
       " '苦+鲁菜',\n",
       " '苦+鲁菜',\n",
       " '咸+东北菜',\n",
       " '咸+东北菜',\n",
       " '咸+川菜',\n",
       " '甜+鲁菜',\n",
       " '苦+东北菜',\n",
       " '咸+鲁菜',\n",
       " '清淡+粤菜',\n",
       " '酸+粤菜',\n",
       " '苦+湘菜',\n",
       " '酸+湘菜',\n",
       " '清淡+湘菜',\n",
       " '甜+沪菜',\n",
       " '咸+湘菜',\n",
       " '酸+沪菜',\n",
       " '苦+东北菜',\n",
       " '苦+粤菜',\n",
       " '辣+鲁菜',\n",
       " '酸+湘菜',\n",
       " '麻辣+川菜',\n",
       " '麻辣+湘菜',\n",
       " '苦+川菜',\n",
       " '酸+鲁菜',\n",
       " '咸+东北菜',\n",
       " '麻辣+川菜',\n",
       " '麻辣+川菜',\n",
       " '咸+湘菜',\n",
       " '咸+粤菜',\n",
       " '辣+粤菜',\n",
       " '酸+粤菜',\n",
       " '咸+沪菜',\n",
       " '辣+东北菜',\n",
       " '麻辣+沪菜',\n",
       " '苦+湘菜',\n",
       " '辣+粤菜',\n",
       " '辣+东北菜',\n",
       " '苦+东北菜',\n",
       " '麻辣+鲁菜',\n",
       " '清淡+川菜',\n",
       " '麻辣+粤菜',\n",
       " '咸+川菜',\n",
       " '辣+沪菜',\n",
       " '咸+湘菜',\n",
       " '苦+东北菜',\n",
       " '辣+鲁菜',\n",
       " '甜+粤菜',\n",
       " '清淡+粤菜',\n",
       " '清淡+粤菜',\n",
       " '麻辣+东北菜',\n",
       " '咸+粤菜',\n",
       " '咸+川菜',\n",
       " '清淡+湘菜',\n",
       " '咸+鲁菜',\n",
       " '麻辣+沪菜',\n",
       " '辣+东北菜',\n",
       " '酸+鲁菜',\n",
       " '辣+粤菜',\n",
       " '辣+湘菜',\n",
       " '咸+鲁菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+东北菜',\n",
       " '苦+川菜',\n",
       " '清淡+粤菜',\n",
       " '甜+沪菜',\n",
       " '咸+粤菜',\n",
       " '咸+湘菜',\n",
       " '辣+东北菜',\n",
       " '麻辣+粤菜',\n",
       " '清淡+东北菜',\n",
       " '麻辣+粤菜',\n",
       " '酸+鲁菜',\n",
       " '酸+鲁菜',\n",
       " '酸+鲁菜',\n",
       " '甜+鲁菜',\n",
       " '酸+东北菜',\n",
       " '甜+湘菜',\n",
       " '酸+湘菜',\n",
       " '咸+粤菜',\n",
       " '清淡+川菜',\n",
       " '咸+沪菜',\n",
       " '苦+鲁菜',\n",
       " '苦+川菜',\n",
       " '辣+东北菜',\n",
       " '甜+粤菜',\n",
       " '清淡+鲁菜',\n",
       " '苦+川菜',\n",
       " '咸+沪菜',\n",
       " '麻辣+沪菜',\n",
       " '清淡+湘菜',\n",
       " '咸+粤菜',\n",
       " '酸+粤菜',\n",
       " '清淡+粤菜',\n",
       " '清淡+湘菜',\n",
       " '咸+川菜',\n",
       " '清淡+湘菜',\n",
       " '辣+湘菜',\n",
       " '辣+湘菜',\n",
       " '甜+川菜',\n",
       " '酸+湘菜',\n",
       " '清淡+川菜',\n",
       " '辣+粤菜',\n",
       " '酸+粤菜',\n",
       " '酸+鲁菜',\n",
       " '酸+川菜',\n",
       " '清淡+鲁菜',\n",
       " '苦+粤菜',\n",
       " '咸+东北菜',\n",
       " '苦+川菜',\n",
       " '酸+东北菜',\n",
       " '苦+东北菜',\n",
       " '清淡+川菜',\n",
       " '甜+川菜',\n",
       " '清淡+沪菜',\n",
       " '苦+沪菜',\n",
       " '苦+川菜',\n",
       " '甜+东北菜',\n",
       " '甜+沪菜',\n",
       " '酸+粤菜',\n",
       " '清淡+湘菜',\n",
       " '酸+湘菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+鲁菜',\n",
       " '辣+东北菜',\n",
       " '清淡+鲁菜',\n",
       " '清淡+湘菜',\n",
       " '麻辣+湘菜',\n",
       " '酸+湘菜',\n",
       " '甜+鲁菜',\n",
       " '清淡+鲁菜',\n",
       " '咸+湘菜',\n",
       " '苦+川菜',\n",
       " '麻辣+沪菜',\n",
       " '酸+川菜',\n",
       " '清淡+川菜',\n",
       " '辣+鲁菜',\n",
       " '甜+东北菜',\n",
       " '麻辣+川菜',\n",
       " '咸+沪菜',\n",
       " '酸+湘菜',\n",
       " '咸+川菜',\n",
       " '苦+沪菜',\n",
       " '辣+东北菜',\n",
       " '苦+沪菜',\n",
       " '咸+湘菜',\n",
       " '酸+鲁菜',\n",
       " '甜+粤菜',\n",
       " '辣+沪菜',\n",
       " '咸+湘菜',\n",
       " '咸+粤菜',\n",
       " '辣+粤菜',\n",
       " '麻辣+川菜',\n",
       " '咸+沪菜',\n",
       " '辣+东北菜',\n",
       " '酸+粤菜',\n",
       " '麻辣+鲁菜',\n",
       " '麻辣+鲁菜',\n",
       " '辣+鲁菜',\n",
       " '清淡+沪菜',\n",
       " '清淡+东北菜',\n",
       " '甜+鲁菜',\n",
       " '麻辣+川菜',\n",
       " '苦+鲁菜',\n",
       " '麻辣+东北菜',\n",
       " '酸+东北菜',\n",
       " '苦+粤菜',\n",
       " '酸+川菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+鲁菜',\n",
       " '苦+粤菜',\n",
       " '酸+粤菜',\n",
       " '辣+湘菜',\n",
       " '辣+湘菜',\n",
       " '苦+湘菜',\n",
       " '苦+川菜',\n",
       " '清淡+鲁菜',\n",
       " '咸+沪菜',\n",
       " '苦+湘菜',\n",
       " '苦+粤菜',\n",
       " '辣+湘菜',\n",
       " '清淡+沪菜',\n",
       " '酸+鲁菜',\n",
       " '麻辣+川菜',\n",
       " '清淡+沪菜',\n",
       " '麻辣+川菜',\n",
       " '酸+湘菜',\n",
       " '辣+粤菜',\n",
       " '苦+粤菜',\n",
       " '甜+鲁菜',\n",
       " '甜+湘菜',\n",
       " '苦+鲁菜',\n",
       " '麻辣+鲁菜',\n",
       " '咸+沪菜',\n",
       " '咸+湘菜',\n",
       " '麻辣+川菜',\n",
       " '辣+粤菜',\n",
       " '咸+沪菜',\n",
       " '清淡+湘菜',\n",
       " '酸+鲁菜',\n",
       " '清淡+鲁菜',\n",
       " '咸+鲁菜',\n",
       " '酸+湘菜',\n",
       " '甜+沪菜',\n",
       " '清淡+沪菜',\n",
       " '咸+川菜',\n",
       " '酸+川菜',\n",
       " '苦+湘菜',\n",
       " '辣+湘菜',\n",
       " '咸+鲁菜',\n",
       " '清淡+粤菜',\n",
       " '苦+沪菜',\n",
       " '咸+沪菜',\n",
       " '苦+川菜',\n",
       " '麻辣+东北菜',\n",
       " '酸+湘菜',\n",
       " '咸+鲁菜',\n",
       " '麻辣+鲁菜',\n",
       " '麻辣+东北菜',\n",
       " '麻辣+鲁菜',\n",
       " '苦+湘菜',\n",
       " '苦+鲁菜',\n",
       " '咸+湘菜',\n",
       " '苦+东北菜',\n",
       " '麻辣+湘菜',\n",
       " '麻辣+沪菜',\n",
       " '辣+川菜',\n",
       " '酸+川菜',\n",
       " '麻辣+东北菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+湘菜',\n",
       " '咸+鲁菜',\n",
       " '甜+粤菜',\n",
       " '清淡+川菜',\n",
       " '甜+粤菜',\n",
       " '辣+东北菜',\n",
       " '咸+粤菜',\n",
       " '咸+川菜',\n",
       " '辣+粤菜',\n",
       " '酸+川菜',\n",
       " '咸+鲁菜',\n",
       " '苦+湘菜',\n",
       " '辣+粤菜',\n",
       " '清淡+沪菜',\n",
       " '苦+粤菜',\n",
       " '酸+沪菜',\n",
       " '辣+湘菜',\n",
       " '咸+沪菜',\n",
       " '咸+湘菜',\n",
       " '苦+川菜',\n",
       " '辣+鲁菜',\n",
       " '清淡+湘菜',\n",
       " '麻辣+沪菜',\n",
       " '清淡+川菜',\n",
       " '苦+川菜',\n",
       " '甜+粤菜',\n",
       " '咸+鲁菜',\n",
       " '清淡+粤菜',\n",
       " '辣+鲁菜',\n",
       " '辣+东北菜',\n",
       " '苦+川菜',\n",
       " '酸+沪菜',\n",
       " '咸+鲁菜',\n",
       " '酸+川菜',\n",
       " '清淡+湘菜',\n",
       " '辣+鲁菜',\n",
       " '辣+川菜',\n",
       " '咸+东北菜',\n",
       " '清淡+粤菜',\n",
       " '苦+湘菜',\n",
       " '辣+东北菜',\n",
       " '咸+湘菜',\n",
       " '清淡+鲁菜',\n",
       " '甜+沪菜',\n",
       " '清淡+湘菜',\n",
       " '甜+湘菜',\n",
       " '甜+川菜',\n",
       " '酸+川菜',\n",
       " '苦+东北菜',\n",
       " '甜+粤菜',\n",
       " '麻辣+东北菜',\n",
       " '甜+鲁菜',\n",
       " '清淡+湘菜',\n",
       " '辣+川菜',\n",
       " '咸+东北菜',\n",
       " '苦+沪菜',\n",
       " '麻辣+鲁菜',\n",
       " '辣+湘菜',\n",
       " '辣+鲁菜',\n",
       " '清淡+湘菜',\n",
       " '苦+东北菜',\n",
       " '酸+东北菜',\n",
       " '甜+川菜',\n",
       " '麻辣+鲁菜',\n",
       " '苦+粤菜',\n",
       " '甜+鲁菜',\n",
       " '麻辣+粤菜',\n",
       " '辣+湘菜',\n",
       " '苦+川菜',\n",
       " '酸+湘菜',\n",
       " '酸+沪菜',\n",
       " '麻辣+粤菜',\n",
       " '麻辣+沪菜',\n",
       " '清淡+东北菜',\n",
       " '酸+沪菜',\n",
       " '苦+沪菜',\n",
       " '咸+鲁菜',\n",
       " '甜+粤菜',\n",
       " '苦+湘菜',\n",
       " '酸+粤菜',\n",
       " '酸+粤菜',\n",
       " '咸+湘菜',\n",
       " '清淡+沪菜',\n",
       " '甜+鲁菜',\n",
       " '咸+东北菜',\n",
       " '清淡+东北菜',\n",
       " '清淡+粤菜',\n",
       " '麻辣+粤菜',\n",
       " '辣+沪菜',\n",
       " '清淡+粤菜',\n",
       " '辣+川菜',\n",
       " '甜+粤菜',\n",
       " '咸+湘菜',\n",
       " '辣+湘菜',\n",
       " '麻辣+沪菜',\n",
       " '辣+湘菜',\n",
       " '咸+湘菜',\n",
       " '甜+东北菜',\n",
       " '辣+沪菜',\n",
       " '咸+川菜',\n",
       " '酸+沪菜',\n",
       " '酸+东北菜',\n",
       " '麻辣+东北菜',\n",
       " '辣+川菜',\n",
       " '清淡+沪菜',\n",
       " '清淡+沪菜',\n",
       " '清淡+川菜',\n",
       " '清淡+川菜',\n",
       " '酸+东北菜',\n",
       " '苦+沪菜',\n",
       " '清淡+粤菜',\n",
       " '酸+湘菜',\n",
       " '麻辣+川菜',\n",
       " '甜+沪菜',\n",
       " '酸+鲁菜',\n",
       " '甜+湘菜',\n",
       " '苦+东北菜',\n",
       " '辣+沪菜',\n",
       " '咸+沪菜',\n",
       " '麻辣+湘菜',\n",
       " '酸+鲁菜',\n",
       " '辣+鲁菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+鲁菜',\n",
       " '甜+鲁菜',\n",
       " '清淡+鲁菜',\n",
       " '清淡+湘菜',\n",
       " '苦+沪菜',\n",
       " '酸+湘菜',\n",
       " '辣+沪菜',\n",
       " '清淡+鲁菜',\n",
       " '酸+沪菜',\n",
       " '麻辣+沪菜',\n",
       " '辣+东北菜',\n",
       " '酸+川菜',\n",
       " '辣+粤菜',\n",
       " '辣+沪菜',\n",
       " '麻辣+沪菜',\n",
       " '清淡+沪菜',\n",
       " '苦+鲁菜',\n",
       " '苦+湘菜',\n",
       " '甜+湘菜',\n",
       " '麻辣+鲁菜',\n",
       " '甜+川菜',\n",
       " '苦+川菜',\n",
       " '咸+沪菜',\n",
       " '甜+鲁菜',\n",
       " '辣+川菜',\n",
       " '麻辣+粤菜',\n",
       " '麻辣+粤菜',\n",
       " '清淡+沪菜',\n",
       " '甜+鲁菜',\n",
       " '清淡+沪菜',\n",
       " '苦+沪菜',\n",
       " '酸+湘菜',\n",
       " '咸+川菜',\n",
       " '清淡+湘菜',\n",
       " '辣+鲁菜',\n",
       " '辣+粤菜',\n",
       " '麻辣+沪菜',\n",
       " '咸+粤菜',\n",
       " '辣+湘菜',\n",
       " '清淡+湘菜',\n",
       " '甜+沪菜',\n",
       " '甜+沪菜',\n",
       " '苦+沪菜',\n",
       " '咸+东北菜',\n",
       " '苦+湘菜',\n",
       " '辣+川菜',\n",
       " '咸+粤菜',\n",
       " '麻辣+湘菜',\n",
       " '清淡+东北菜',\n",
       " '清淡+川菜',\n",
       " '清淡+湘菜',\n",
       " '辣+沪菜',\n",
       " '苦+粤菜',\n",
       " '苦+粤菜',\n",
       " '酸+东北菜',\n",
       " '苦+沪菜',\n",
       " '甜+川菜',\n",
       " '酸+鲁菜',\n",
       " '辣+东北菜',\n",
       " '辣+沪菜',\n",
       " '咸+粤菜',\n",
       " '辣+东北菜',\n",
       " '麻辣+粤菜',\n",
       " '辣+川菜',\n",
       " '麻辣+东北菜',\n",
       " '麻辣+沪菜',\n",
       " '清淡+沪菜',\n",
       " '酸+沪菜',\n",
       " '苦+鲁菜',\n",
       " '苦+东北菜',\n",
       " '甜+鲁菜',\n",
       " '酸+鲁菜',\n",
       " '苦+川菜',\n",
       " '麻辣+东北菜',\n",
       " '麻辣+湘菜',\n",
       " '甜+川菜',\n",
       " '辣+川菜',\n",
       " '苦+沪菜',\n",
       " '甜+粤菜',\n",
       " '酸+湘菜',\n",
       " '清淡+沪菜',\n",
       " '酸+粤菜',\n",
       " '麻辣+粤菜',\n",
       " '辣+粤菜',\n",
       " '苦+东北菜',\n",
       " '辣+鲁菜',\n",
       " '甜+鲁菜',\n",
       " '麻辣+东北菜',\n",
       " '麻辣+湘菜',\n",
       " '辣+沪菜',\n",
       " '甜+鲁菜',\n",
       " '清淡+川菜',\n",
       " '辣+川菜',\n",
       " '咸+鲁菜',\n",
       " '苦+东北菜',\n",
       " '咸+川菜',\n",
       " '甜+湘菜',\n",
       " '清淡+粤菜',\n",
       " '麻辣+鲁菜',\n",
       " '苦+湘菜',\n",
       " '麻辣+湘菜',\n",
       " '清淡+湘菜',\n",
       " '苦+沪菜',\n",
       " '咸+沪菜',\n",
       " '清淡+湘菜',\n",
       " '辣+东北菜',\n",
       " '咸+湘菜',\n",
       " '甜+湘菜',\n",
       " '甜+川菜',\n",
       " '辣+东北菜',\n",
       " '酸+川菜',\n",
       " '辣+沪菜',\n",
       " '清淡+川菜',\n",
       " '麻辣+沪菜',\n",
       " '麻辣+粤菜',\n",
       " '咸+粤菜',\n",
       " '麻辣+湘菜',\n",
       " '酸+川菜',\n",
       " '咸+川菜',\n",
       " '辣+鲁菜',\n",
       " '清淡+沪菜',\n",
       " '苦+粤菜',\n",
       " '苦+沪菜',\n",
       " '咸+粤菜',\n",
       " '麻辣+沪菜',\n",
       " '甜+湘菜',\n",
       " '咸+东北菜',\n",
       " '酸+粤菜',\n",
       " '清淡+东北菜',\n",
       " '酸+东北菜',\n",
       " '麻辣+湘菜',\n",
       " '辣+沪菜',\n",
       " '辣+湘菜',\n",
       " '酸+川菜',\n",
       " '清淡+东北菜',\n",
       " '咸+川菜',\n",
       " '苦+东北菜',\n",
       " '酸+粤菜',\n",
       " '麻辣+粤菜',\n",
       " '咸+沪菜',\n",
       " '酸+湘菜',\n",
       " '甜+川菜',\n",
       " '辣+鲁菜',\n",
       " '咸+川菜',\n",
       " '甜+粤菜',\n",
       " '咸+粤菜',\n",
       " '清淡+川菜',\n",
       " '酸+鲁菜',\n",
       " '咸+鲁菜',\n",
       " '麻辣+鲁菜',\n",
       " '辣+川菜',\n",
       " '辣+沪菜',\n",
       " '辣+粤菜']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_user_features(num_users=1000):\n",
    "    taste_options = ['甜', '咸', '酸', '辣', '麻辣', '苦', '清淡']\n",
    "    cuisine_options = ['川菜', '湘菜', '粤菜', '东北菜', '沪菜', '鲁菜']\n",
    "    features = cuisine_options + taste_options  # 可选：仅用于调试输出\n",
    "    print(f\"All available features: {features}\")\n",
    "    \n",
    "    user_features = []\n",
    "    for _ in range(num_users):\n",
    "        # 随机选择一个口味和一个菜系组合\n",
    "        taste = random.choice(taste_options)\n",
    "        cuisine = random.choice(cuisine_options)\n",
    "        user_feature = f\"{taste}+{cuisine}\"  # 例如 \"甜+川菜\"\n",
    "        user_features.append(user_feature)\n",
    "    return user_features\n",
    "\n",
    "\n",
    "user_features=generate_user_features()\n",
    "user_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1024)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feature=[]\n",
    "for i in user_features:\n",
    "    user_feature.append(embeddings.embed_query(i))\n",
    "\n",
    "user_features=np.array(user_feature)\n",
    "user_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>dish_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>9007</td>\n",
       "      <td>3.739346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>963</td>\n",
       "      <td>13817</td>\n",
       "      <td>4.820340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>849</td>\n",
       "      <td>8314</td>\n",
       "      <td>2.068983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>373</td>\n",
       "      <td>10844</td>\n",
       "      <td>4.756350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>545</td>\n",
       "      <td>2244</td>\n",
       "      <td>0.049566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>249</td>\n",
       "      <td>11777</td>\n",
       "      <td>4.320426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>322</td>\n",
       "      <td>10030</td>\n",
       "      <td>3.845429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>793</td>\n",
       "      <td>6630</td>\n",
       "      <td>4.469949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>810</td>\n",
       "      <td>10087</td>\n",
       "      <td>4.694995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>970</td>\n",
       "      <td>505</td>\n",
       "      <td>1.180745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  dish_id     score\n",
       "0          36     9007  3.739346\n",
       "1         963    13817  4.820340\n",
       "2         849     8314  2.068983\n",
       "3         373    10844  4.756350\n",
       "4         545     2244  0.049566\n",
       "...       ...      ...       ...\n",
       "9995      249    11777  4.320426\n",
       "9996      322    10030  3.845429\n",
       "9997      793     6630  4.469949\n",
       "9998      810    10087  4.694995\n",
       "9999      970      505  1.180745\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# 交互数据生成\n",
    "def generate_interaction_data(num_interactions=10000, num_users=1000, num_dishes=15000):\n",
    "    interaction_data = {\n",
    "        'user_id': np.random.randint(0, num_users, size=num_interactions),  # 随机用户ID (0-999)\n",
    "        'dish_id': np.random.randint(0, num_dishes, size=num_interactions),  # 随机菜品ID (0-9)\n",
    "        'score': np.random.uniform(0, 5, size=num_interactions)  # 随机评分 (0-5)\n",
    "    }\n",
    "    return pd.DataFrame(interaction_data)\n",
    "\n",
    "a=generate_interaction_data()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTensor(\n",
      "  context=Context(features={}, sizes=[1], shape=(), indices_dtype=tf.int32),\n",
      "  node_set_names=['user', 'dish'],\n",
      "  edge_set_names=['interacts', 'interacts_reverse'])\n",
      "WARNING:tensorflow:From d:\\NLP\\DPR1\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 0, Loss: 13757.9893\n",
      "Epoch 10, Loss: 129580318720.0000\n",
      "Epoch 20, Loss: 3897216512.0000\n",
      "Epoch 30, Loss: 401284896.0000\n",
      "Epoch 40, Loss: 100179952.0000\n",
      "Epoch 50, Loss: 90656368.0000\n",
      "Epoch 60, Loss: 41658720.0000\n",
      "Epoch 70, Loss: 5790555.0000\n",
      "Epoch 80, Loss: 3146791.7500\n",
      "Epoch 90, Loss: 1802506.6250\n",
      "Top 5 recommended dishes for user 1: ['甜味', '甜味', '甜味', '甜味', '甜味']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# 数据准备\n",
    "def prepare_graph_tensor(user_features, dish_features, interaction_df):\n",
    "    graph = tfgnn.GraphTensor.from_pieces(\n",
    "        node_sets={\n",
    "            'user': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([user_features.shape[0]]),\n",
    "                features={'hidden_state': tf.convert_to_tensor(user_features, dtype=tf.float32)}\n",
    "            ),\n",
    "            'dish': tfgnn.NodeSet.from_fields(\n",
    "                sizes=tf.constant([dish_features.shape[0]]),\n",
    "                features={'hidden_state': tf.convert_to_tensor(dish_features, dtype=tf.float32)}\n",
    "            )\n",
    "        },\n",
    "        edge_sets={\n",
    "            'interacts': tfgnn.EdgeSet.from_fields(\n",
    "                sizes=tf.constant([len(interaction_df)]),\n",
    "                adjacency=tfgnn.Adjacency.from_indices(\n",
    "                    source=('user', tf.convert_to_tensor(interaction_df['user_id'].values, dtype=tf.int32)),\n",
    "                    target=('dish', tf.convert_to_tensor(interaction_df['dish_id'].values, dtype=tf.int32))\n",
    "                ),\n",
    "                features={'score': tf.convert_to_tensor(interaction_df['score'].values, dtype=tf.float32)}\n",
    "            ),\n",
    "            'interacts_reverse': tfgnn.EdgeSet.from_fields(\n",
    "                sizes=tf.constant([len(interaction_df)]),\n",
    "                adjacency=tfgnn.Adjacency.from_indices(\n",
    "                    source=('dish', tf.convert_to_tensor(interaction_df['dish_id'].values, dtype=tf.int32)),\n",
    "                    target=('user', tf.convert_to_tensor(interaction_df['user_id'].values, dtype=tf.int32))\n",
    "                ),\n",
    "                features={'score': tf.convert_to_tensor(interaction_df['score'].values, dtype=tf.float32)}\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    return graph\n",
    "\n",
    "# 模型定义\n",
    "class BipartiteGNN(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(BipartiteGNN, self).__init__()\n",
    "        self.user_conv = tfgnn.keras.layers.GraphUpdate(\n",
    "            node_sets={'user': tfgnn.keras.layers.NodeSetUpdate(\n",
    "                edge_set_inputs={'interacts_reverse': tfgnn.keras.layers.SimpleConv(\n",
    "                    sender_node_feature='hidden_state',\n",
    "                    message_fn=tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )},\n",
    "                next_state=tfgnn.keras.layers.NextStateFromConcat(\n",
    "                    tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )\n",
    "            )}\n",
    "        )\n",
    "        self.dish_conv = tfgnn.keras.layers.GraphUpdate(\n",
    "            node_sets={'dish': tfgnn.keras.layers.NodeSetUpdate(\n",
    "                edge_set_inputs={'interacts': tfgnn.keras.layers.SimpleConv(\n",
    "                    sender_node_feature='hidden_state',\n",
    "                    message_fn=tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )},\n",
    "                next_state=tfgnn.keras.layers.NextStateFromConcat(\n",
    "                    tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "                )\n",
    "            )}\n",
    "        )\n",
    "        self.user_proj = tf.keras.layers.Dense(output_dim)\n",
    "        self.dish_proj = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "    def call(self, graph):\n",
    "        graph = self.user_conv(graph)\n",
    "        user_emb = self.user_proj(graph.node_sets['user']['hidden_state'])\n",
    "        graph = self.dish_conv(graph)\n",
    "        dish_emb = self.dish_proj(graph.node_sets['dish']['hidden_state'])\n",
    "        return user_emb, dish_emb\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(model, graph):\n",
    "    user_emb, dish_emb = model(graph)\n",
    "    edge_indices = graph.edge_sets['interacts'].adjacency\n",
    "    user_idx = edge_indices.source\n",
    "    dish_idx = edge_indices.target\n",
    "    pred_scores = tf.reduce_sum(\n",
    "        tf.gather(user_emb, user_idx) * tf.gather(dish_emb, dish_idx),\n",
    "        axis=1\n",
    "    )\n",
    "    true_scores = graph.edge_sets['interacts']['score']\n",
    "    return tf.reduce_mean(tf.keras.losses.mean_squared_error(true_scores, pred_scores))\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, graph, epochs=10000, lr=0.01):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(model, graph)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.numpy():.4f}')\n",
    "    return model\n",
    "\n",
    "# 推荐系统\n",
    "def get_recommendations(model, graph, user_id, top_k=5):\n",
    "    user_emb, dish_emb = model(graph)\n",
    "    user_vec = tf.nn.l2_normalize(user_emb[user_id], axis=0)\n",
    "    dish_emb = tf.nn.l2_normalize(dish_emb, axis=1)\n",
    "    scores = tf.matmul(dish_emb, tf.expand_dims(user_vec, 1))[:, 0]\n",
    "    top_indices = tf.argsort(scores, direction='DESCENDING')[:top_k]\n",
    "    return top_indices.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "\n",
    "    \n",
    "    graph_tensor = prepare_graph_tensor(user_features, dish_feature, a)\n",
    "    print(graph_tensor)\n",
    "    model = BipartiteGNN(512, 128)\n",
    "    trained_model = train_model(model, graph_tensor)\n",
    "    \n",
    "    user_id = 1\n",
    "    recommendations = get_recommendations(trained_model, graph_tensor, user_id)\n",
    "    dish=[]\n",
    "    for i in recommendations:\n",
    "         dish.append(dish_features[i])\n",
    "\n",
    "    print(f\"Top 5 recommended dishes for user {user_id}: {dish}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
